# 模型应用的功能性测试

### 样本特征质量的评估

### 模型质量的评估

### 模型在线预估服务的质量保障

- 小样本离线在线打分对比

在模型上线正式服务之前，需要对模型进行测试验证，除了准备常规的test数据集，还需要单独分离出一部分样本集，称之为小样本数据集。通过小样本数据集在线系统的得分与离线分数的对比的差异，验证模型的在线服务质量。有点类似灰度验证！

![](./pictures/小样本测试.png)

# 模型实时更新链路测试
假如在10-15分钟时间间隔里，在线服务的模型会被更替为新的模型，那么留给模型验证的时间最多只有10分钟时间，这也是online deep learning带来的质量测试挑战。对于这个问题，有以下两种解决方案：

(1)建立online learning全链路质量指标监控体系。这里的链路指的是从样本构建到在线预测的全链路，包括样本域的指标校验和训练域指标校验。

**\*\*\*\*\* 具体方法 \*\*\*\*\***

比如计算在测试集上的AUC或者分组AUC。那么测试集如何选取？建议测试集除了取下一个时间窗口的数据（**必须是未见过的数据**）,还可以包含从过去一段时间（比如一周）的数据中随机抽样一部分数据（**测试模型是否跑偏**）。
