### 集成学习

之前已经从理论角度分析过集成学习算法族，见[集成学习算法](https://zhuanlan.zhihu.com/p/97200320)，以及LightGBM性能优化[LightGBM性能优化](https://zhuanlan.zhihu.com/p/102281499)。这次将从工程实践的角度来分析如何使用它们

### 方差与偏差分析

Bagging策略是对样本进行重采样，对每一个重采样得到的子样本集训练一个模型，最后取平均。由于子样本集合的相似性以及使用的是相同模型，得到的每个模型具有相同的均值与方差（相同的bias与variance）。由于：

$$E\[\frac{\sum{X_i}}{n}\]=E\[X_i\]$$，所以bagging后的子模型与单个模型一致，并不能显著降低bias。另一方面，**当各个子模型完全独立时，则有**$$Var(\frac{\sum{X_i}}{n})=\frac{Var(X_i)}{n}$$。当各个模型完全一致时，即所有模型完全相关，得到$$Var(\frac{\sum{X_i}}{n})={Var(X_i)}$$

而bagging策略得到的模型虽说不是完全独立，但也不是完全相关的。所以bagging得到的模型方差应该介于两者之间，可以一定程度上减小方差。**为了进一步降低方差，随机森林会对特征进行随机采样，使用特征子集建立每个树模型**。这样以来，使得每棵树训练使用的特征尽量不一致，减小每个模型之间的相关性。另外，使用特征子集训练得到的树模型也会起到避免过拟合的作用。


