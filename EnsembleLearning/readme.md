### 集成学习

之前已经从理论角度分析过集成学习算法族，见[集成学习算法](https://zhuanlan.zhihu.com/p/97200320)，以及LightGBM性能优化[LightGBM性能优化](https://zhuanlan.zhihu.com/p/102281499)。这次将从工程实践的角度来分析如何使用它们

### 方差与偏差分析

偏差是指由有所采样得到的大小为m的训练数据集，训练出的所有模型的输出的平均值和真实模型输出之间的偏差。方差是指有所有采样得到的大小为m的训练数据集，训练出的所有模型的输出的方差

Boosting、Bagging与Stacking都是聚合的方法，应该视不同的情况使用不同的方法，如果我们选择具有低偏置高方差的基础模型，我们应该使用一种倾向于减小方差的聚合方法；而如果我们选择具有低方差高偏置的基础模型，我们应该使用一种倾向于减小偏置的聚合方法。

- Bagging

Bagging策略是对样本进行重采样，对每一个重采样得到的子样本集训练一个模型，最后取平均。由于子样本集合的相似性以及使用的是相同模型，得到的每个模型具有相同的均值与方差（相同的bias与variance）。由于：

$$E\[\frac{\sum{X_i}}{n}\]=E\[X_i\]$$，所以bagging后的子模型与单个模型一致，并不能显著降低bias。另一方面，**当各个子模型完全独立时，则有**$$Var(\frac{\sum{X_i}}{n})=\frac{Var(X_i)}{n}$$。当各个模型完全一致时，即所有模型完全相关，得到$$Var(\frac{\sum{X_i}}{n})={Var(X_i)}$$。而bagging策略得到的模型虽说不是完全独立，但也不是完全相关的。所以bagging得到的模型方差应该介于两者之间，可以一定程度上减小方差。**为了进一步降低方差，随机森林会对特征进行随机采样，使用特征子集建立每个树模型**。这样以来，使得每棵树训练使用的特征尽量不一致，减小每个模型之间的相关性。另外，使用特征子集训练得到的树模型也会起到避免过拟合的作用。

既然Bagging算法能够有效的降低方差，无法降低偏差。我们训练时，使用的基模型应该选择尽量小的偏差。**所以，随机森林训练得到的树模型一般要求要较深一点**

- Boosting

而对于Boosting算法，采用的是前向分步算法。每一步都是基于前面时间步的树模型，sequential的优化损失函数。我们得到的每个模型是强相关的，所以Boosting不能显著的降低方差，能降低偏差。

既然Boosting算法可以显著的降低偏差，无法降低方差，那么我们boosting算法训练的基模型应该选择尽量小的方差，否则我们最后得到的boosting模型方差很大。这就需要我们限制树模型的复杂度，避免基模型过拟合。

- Stacking

异质集成，并且通过学习一个元模型将这些模型组织起来。

### CART学习方法

### GBDT学习步骤

- 学习流程

- 模型初始化(对数几率)

- GBDT为什么是拟合对数几率

### Boosting算法参数

- Shrinkage(学习率learning_rate)

每走一小步逐渐逼近结果的效果要比每次迈一大步很快逼近结果的方式更容易避免过拟合，即它并不是完全信任每一棵树。而是会给没棵树一个权重。把大步切成小步。本质上 Shrinkage 为每棵树设置了一个 weight，累加时要乘以这个 weight，当 weight 降低时，基模型数会配合增大

- 控制过拟合

    - 步长学习率的设置
    
    - 自采样比例，无放回的抽样
    
    - 剪枝，限制树模型的复杂程度
    
- XGBoost

    - Boosting集成架构参数
    
        - booster：CART、或者线性模型、或者DART
        
        - objective：任务目标种类

    - 弱学习器参数
  
        - max_depth:树的深度
    
        - min_child_weight:最小子节点的权重。如果某个子节点权重小于这个阈值，则不会在分裂。使用的是该节点所有二阶导数的和
    
        - gamma：分裂所带来的损失最小阈值，大于此值，才能继续分裂
        
        - subsample：子采样参数，无放回抽样。每棵树的学习使用子采样的样本集合进行训练。
        
        - reg_alpha：L1正则化参数
        
        - reg_lambda： L2正则化参数


