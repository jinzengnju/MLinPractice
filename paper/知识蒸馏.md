# 知识蒸馏

什么是知识蒸馏，简而言之就是模型压缩。Bert在各个领域效果好已经不用多说，但是在实际生产环境下，Bert往往不能直接放到线上使用。知识蒸馏的提出，就是为了将复杂的模型压缩，以便线上性能能够抗住。

最早的论文可见：

Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. "Distilling the knowledge in a neural network." *arXiv preprint arXiv:1503.02531* (2015).

​		大致思想是：可以先训练出一个复杂的teacher网络，然后将teacher网络的输出结果q作为student网络的目标，训练student网络，使得student网络的结果p能够尽可能接近q。因此网络损失函数为：
$$
L=CE(y,p)+\alpha*CE(p,q)
$$
​		这里的y是student网络真实的标签，q是teacher网络的输出结果，p是student网络的输出结果，而交叉熵CE就是衡量了两个概率分布之间的距离。但是有一个问题，这里teacher网络输出的q是否就是softmax的输出呢？论文在得到teacher网络的时候，做了以下运算：
$$
q_i=\frac{exp(z_i/T)}{\sum _j (z_j/T)}
$$
​		这里的 $q_i$ 就是student网络学习的对象（soft targets），$z_i$ 是teacher网络softmax前的输出。如果将T取1，这个公式就是softmax，根据logit输出各个类别的概率。如果T接近于0，则最大的值会越近1，其它值会接近0，近似于onehot编码。如果T越大，则输出的结果的分布越平缓，相当于平滑的一个作用，起到保留相似信息的作用。如果T等于无穷，就是一个均匀分布。

​		想象有这样一个问题，在数字识别中，数字2与3、7从外形上来看比较相似，但是如果在得到q时只简单的用softmax，会造成对于某个图像“2”的输入，2预测的概率非常高，而3和7的概率将接近0。这样的话，由于它们的概率值接近0，teacher网络学到数据的相似信息（例如数字2和3，7很类似）很难传达给student网络。



## 实例

#### 模型结构

Tang, Raphael, et al. "Distilling task-specific knowledge from bert into simple neural networks." *arXiv preprint arXiv:1903.12136* (2019).

假设现在需要解决两个问题：1.输入一句话进行分类 2.输入两句话，判断句子是否匹配

解决方案：预先训练Bert模型，任务为Masked LM与NSP来做pretrain，对于情况1，需要我们finetune一个Bert模型用于分类，而对于情况2可以不用再进行pretrain。那么对于情况1与情况2如何设计student网络呢？

情况1：

![](知识蒸馏/stu1.jpg)

情况2：需要对情况2进行简单说明一下，进行特征融合时，取两个句子最后一步的biLSTM的hidden states $h_{s1}，h_{s2}$， 然后拼接为 $[h_{s1}, h_{s2}, h_{s1}h_{s2},|h_{s1}-h_{s2}|]$ 作为全连接层的输入，然后经过softmax得到匹配结果。

![](知识蒸馏/2.jpg)

#### 损失函数

假设 $z=z^{(S)}$ 为student网络的输出，$z^{(B)}$为teacher网络的输出，那么损失可以按照如下计算：

![](知识蒸馏/3.png)

student网络计算的是CE损失，而蒸馏损失为均方误差MSE

## 注意

* 知识蒸馏，可以用来将网络从大网络转化成一个小网络，并保留接近于大网络的准确率
* 也可以将多个网络的学到的知识转移到一个网络中，使得单个网络的性能接近emsemble的结果。

