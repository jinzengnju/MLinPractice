# 迁移学习

主要步骤如下：

* 从预训练的模型中获取相关层，并加载对应的权重
* Freeze这些层，避免在后续训练中破坏已经学习到的知识
  * 如果不冻结，将随机初始化的layers与预训练的layers混合后，随机初始化的层可能会导致模型具有较大的梯度（开始是不准的），并随之破坏了之前训练好的feature
* Add some new, trainable layers on top of the frozen layers. They will learn to turn the old features into predictions on a new dataset
* Train the new layers on your dataset.

执行完上述步骤之后，在最后可选择性的执行finetune。在这一步，**你需要解冻前面的冻结层，并且在新的训练数据基础上，以一个非常小的学习率，重新训练模型。**

## 如何设置参数不可训练？

对于keras的模型，非常简单的就能设置模型、layer参数是否可训练。

~~~python
layer = keras.layers.Dense(3)
layer.build((None, 4))  # Create the weights
layer.trainable = False  # Freeze the layer

print("weights:", len(layer.weights))
print("trainable_weights:", len(layer.trainable_weights))
print("non_trainable_weights:", len(layer.non_trainable_weights))

#weights: 2
#trainable_weights: 0
#non_trainable_weights: 2
~~~

## 典型的迁移学习工作流程

* 方法一：
  * Instantiate a base model and load pre-trained weights into it.
  * Freeze all layers in the base model by setting `trainable = False`
  * Create a new model on top of the output of one (or several) layers from the base model.
  * Train your new model on your new dataset.
  
  [案例](./transfer.py)
  
* 方法二：feature extraction
  * Instantiate a base model and load pre-trained weights into it.
  * 利用训练好的base model，**得到关于新数据的feature map**
  * 将得到的feature map，输入到新的分类器，训练新模型

对于上述两种方法，方法而可以类似ELMO动态的调整输入，即最后一层分类器的输入特征是变化的。



## FineTune

当你迁移学习的模型在新的数据集合上收敛时，便可以进行finetune。先unfreeze所有或者部分的base model，用**较小的学习率重新训练一个端到端的模型。**但是，这有可能造成模型过拟合，训练的时候应该注意。

**It's also critical to use a very low learning rate at this stage, because you are training a much larger model than in the first round of training, on a dataset that is typically very small. As a result, you are at risk of overfitting very quickly if you apply large weight updates. Here, you only want to readapt the pretrained weights in an incremental way.**

非常重要的说三遍：

**It is critical to only do this step *after* the model with frozen layers has been trained to convergence.**

**It is critical to only do this step *after* the model with frozen layers has been trained to convergence.**

**It is critical to only do this step *after* the model with frozen layers has been trained to convergence.**

为什么transfer learning的时候需要冻结pretrain的模型？

**It is critical to only do this step *after* the model with frozen layers has been trained to convergence. If you mix randomly-initialized trainable layers with trainable layers that hold pre-trained features, the randomly-initialized layers will cause very large gradient updates during training, which will destroy your pre-trained features.**

* 实例

~~~python
# Unfreeze the base model
base_model.trainable = True

# It's important to recompile your model after you make any changes
# to the `trainable` attribute of any inner layer, so that your changes
# are take into account
model.compile(optimizer=keras.optimizers.Adam(1e-5),  # Very low learning rate
              loss=keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=[keras.metrics.BinaryAccuracy()])

# Train end-to-end. Be careful to stop before you overfit!
model.fit(new_dataset, epochs=10, callbacks=..., validation_data=...)
~~~

**Calling `compile()` on a model is meant to "freeze" the behavior of that model. This implies that the `trainable` attribute values at the time the model is compiled should be preserved throughout the lifetime of that model, until `compile` is called again. Hence, if you change any `trainable` value, make sure to call `compile()` again on your model for your changes to be taken into account.**

* finetune中的**BatchNormalization**

  finetune对包含BatchNormalization的base model进行unfreeze时，应该始终保证BatchNormalization在finetune的过程中始终处于inference mode，（training=False）。Otherwise the updates applied to the non-trainable weights will suddenly destroy what the model has learned.

## Transfer learning & fine-tuning with a custom training loop

~~~python
base_model = keras.applications.Xception(
    weights="imagenet",  # Load weights pre-trained on ImageNet.
    input_shape=(150, 150, 3),
    include_top=False,
)  # Do not include the ImageNet classifier at the top.

# Freeze the base_model
base_model.trainable = False

# Create new model on top
inputs = keras.Input(shape=(150, 150, 3))
x = data_augmentation(inputs)  # Apply random data augmentation

# Pre-trained Xception weights requires that input be normalized
# from (0, 255) to a range (-1., +1.), the normalization layer
# does the following, outputs = (inputs - mean) / sqrt(var)
norm_layer = keras.layers.experimental.preprocessing.Normalization()
mean = np.array([127.5] * 3)
var = mean ** 2
# Scale inputs to [-1, +1]
x = norm_layer(x)
norm_layer.set_weights([mean, var])

# The base model contains batchnorm layers. We want to keep them in inference mode
# when we unfreeze the base model for fine-tuning, so we make sure that the
# base_model is running in inference mode here.
x = base_model(x, training=False)
x = keras.layers.GlobalAveragePooling2D()(x)
x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout
outputs = keras.layers.Dense(1)(x)
model = keras.Model(inputs, outputs)

model.summary()
~~~

x = base_model(x, training=False)确保了BatchNormalization处于inference状态